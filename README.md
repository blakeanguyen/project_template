# Research Practicum 2018 – Research Plan

> * Group Name: Mr. Robot
> * Group participants names: Blake Nguyen and Na'Kiya Russell
> * Project Title: Project Title: Human Robot Interaction (HRI): Trust and Reliance in Diagnostic Aiding Automation

## Purpose Statement
The purpose of this study is to explore the reliability threshold and its effect on human-robot teams in dangerous and safe environments. The study also seeks to compare at what level of expertise: novice or expert, relies more on diagnostic aiding technology (i.e., autonomous agent, automation) in such environments.


## General Introduction

In human robot teams, there has been an increase in the use of automation being used to help teams operate tasks effectively. There any many environments where teams may have the information they need or a disruption of unexpected events occurs where unforeseen events takes place; in either situation it is best to be prepared. One factor we wanted to research is trust in diagnostic aiding automation (robots). There is a type of automation called diagnostic aiding, which involves information acquisition and analysis. When diagnostic aiding falls below 70% reliability, it is seen as not useful, and will negatively affect overall task performance. Along with a diagnostic aid, a reliability threshold was developed to measure tasks humans are capable of performing without needing help from such technologies. Furthermore, it is stated that regardless of the reliability threshold, humans will still rely on diagnostic aiding automation in dangerous environments. In benign (safe) environmental conditions, the reliability level isn’t as high but it is still noted to be prominent. There is a respective interest in further investigating whether the level of expertise or background knowledge can play a factor in how or when humans decide to rely on diagnostic aiding automation.

The problem that we are tackling is identity in what combinations of the environments (benign or dangerous), reliability threshold (80% or 50%), and level of expertise (novice or expert) will a human rely on a diagnostic aid to help guide their situational awareness when they are unable too.

(Motivation. Why is this problem interesting and relevant to the research community?)

This is an interesting problem because we can understand in what environments, at what level of reliability threshold, and expertise level do humans tend to rely on technology. This can potentially benefit future research by identifying at what level of reliability does technology need to developed at (or near) to make each group (novice or expert) successful in their own right no matter the environment.

(Proposed Solution. How do we propose to tackle this problem (that has been identified in the previous paragraphs, is interesting to the community, and has yet to be tackled by other researchers)?)

We propose to tackle the problem by conduct the research/experiment on novice and experts in benign and dangerous environments with a reliability threshold of 80% and 50%??

(Contributions. An enumeration of the contributions of the senior design project)

(This project makes the following contributions:)(you must have this!!)
•	We believe our project will contribute to futher advancing research regarding human-robot interaction (HRI).
•	We believe our project will contribute to the area of research that is interested in reliability in automation across human-robot teams.
•	We believe our project will provide a better understanding of reliability and willingness to work with automation at various levels of expertise.


## Related Work

Discuss briefly about published matter that technically relates to your proposed work in 4-5 paragraphs.



Literature Review - The Human-Automation Relationship
As tasks become more difficult for people to undertake, automation proves to be a promising alternative to decrease physical and mental workload. For the most part, automation take most complicated tasks and simplify them to aid human labor, or replace human labor altogether. People are prone to errors; automation significantly decreases human-made errors and increase efficiency. However, complete reliability is rarely achieved, and trust dwindles as reliability decreases. Nonetheless, there have been huge strides with every newer technology being developed; for example, researchers of different disciplines use automation to discover new marine or extraterrestrial life forms. Complex surgical procedures are possible with the use of automations. In spite of technical inaccuracy or other types of error during human-automation interaction, automations have progressed a plethora of technological advancements to human-robot teams.


Automation Errors. The human-automation relationship contain many variables that could affect the overall performance of a set task. Dixon, Wickens, & McCarley (2007) wanted to make a clear distinction between the effects that false alarms and misses have on operators’ trust in automation. They focused on two types of automation errors: false alarms and misses. They had their participants undertake two tasks that involved aid of an automation: a tracking task and a monitoring task. Among the hypotheses of Dixon et al. (2007), believed that false alarms would harm performance by decreasing compliance and reliance. False alarms would decrease compliance by having operators distrust and ignore some of the alerts. According to the results, operators shifted their attention away from the tracking task (thereby reducing that task performance), and monitor the system’s gauge (Dixon, Wickens, & McCarley, 2007).


Dixon et al. also concluded that misses had no significant effect on compliance. It was thought that compliance and reliance were two independent constructs that can be reduced by two different automation errors. False alarms would affect only compliance. Misses would only affect reliance (Dixon, Wickens, & McCarley, 2007). However, Dixon et al. (2007), Stephen Rice (2008) provided evidence that  false alarms and misses can decrease the magnitude of both constructs, with one to a lesser degree. Rice (2008) used multiple automation aids in his study to further understand those constructs as previous models of their relationship did not seem sufficient enough.


Signal Detection Theory. Wickens and Colcombe (2007) also experimented with the two constructs with a similar method, however, he offered a different outlook on the relationship between the two. Within the scope of signal detection theory, human-automation interaction is affected by the automation’s reliability (discriminate between nonevent and event) and response bias (alert and silence). In this case, Rice was focusing on the automation’s contribution in the dual tasking, and how it affects overall performance. His study supported previous claims of false alarms and misses being detrimental to task performance by lowering reliance and compliance. He explained that while both errors occurs, operators would have to divert their attention to a different task in order to double check their automation aid. This process takes time way from their assigned task and, therefore, reduce efficiency (Wickens and Colcombe, 2007). Participants interestingly continued to depend on their automation aids despite the lowered reliability. As for the automation’s response bias, results showed evidence of auditory alerts being useful for two different concurrent tasks. They’re not easy to ignore, and they require a different perceptual modality. However, different types of alerts in a false alarmed prone setting, degraded performance significantly.


Reliability Disclosure. According to Wang et al. (2009), when automations aids are set to complete reliability, participants still had some distrust in them, as results showed incomplete reliance.  The objective of the study was to determine how participants appropriated their reliance on automation aids when their reliability levels were made known beforehand. Results show that while the informed group adjusted to the disclosed reliability setting, they still underperformed than expected. Apparently, this fits in line with the “sluggish beta” phenomenon, where participants don’t adjust their responses ideally when relying on the combat identification aid.


Single vs. Multiple Aids. Keller and Rice (2010) shifted the focus from single aid augmentation to an automation-human dynamic where multiple aids are present. Prior research have studied the paradigm of trust and single aid setting. In this study, the objective was to determine which multiple-trust theory holds more promise. Data supported the system-wide trust theory in which operators put trust in the overall system of automation aids. Rather than having varied levels of trust, or reliance, with each aid individually (as in component-specific trust theory), operators appoint the same amount of trust for each aid despite differences in reliability (Keller D. and Rice S., 2010).


Adaptive Automation. Level and type of automation was also considered to be an issue with multiple-UV systems (de Visser & Parasuraman, 2011). There are frameworks in determining how much, on what, and when to automate. Automation involvement can range from doing no work or doing all tasks. Automations can augment tasks concerning “information acquisition, information analysis, decision making, and decision implementation” (de Visser & Parasuraman, 2011). Automation could assist in critical situations, when operators’ cognitive state comes into question, or a combination of both. There would be a risk of unpredictability, but giving operators’ delegation of tasks relieve it. Contrary to previous experimental results, in a multi-UV system, imperfect automation proved to be beneficial in reliability rates as low as 30% depending on the type of target detection task (de Visser & Parasuraman, 2011). In the second experiment, adaptive automation was seen to as more useful than static automation, where operators gave the adaptive automation a high trust rating when it intervened at the opportune moment. In this case, the automation intervened when task load was high.


While there were setbacks with automation aids, there have been documented benefits and stability with human-automation interactions. Levels of trust is an important indicator of operators’ dependence of automations. The ability to perceive patterns and orientation relative to other objects and the switch attention from one task to another are important markers, too. Newer research are focusing on how many automations operators can oversee, with numbers ranging from one to sixteen. Research on adaptive automation suggests that operators’ tend to trust automations when workload is heavy and when the automation intervenes at the most critical event. Major work has been done to definitively establish the relationship between compliance and reliance, however, limitations in experiments restrict the process. Nonetheless, both constructs seem to be two different forms of trust, and they are both affected by two different automation errors.




## Research Questions
* Is there a difference in perception of reliable and unreliable diagnostic aiding automation in both safe and dangerous environments when comparing levels of expertise (novice and experts)? 
* Do novice consider diagnostic aiding automation to be reliable or unreliable as opposed to an expert operator when automation reliability rate is set to 50% in both benign and hostile environments?

## Research Methods

### Study Design

![current study design.png](./current study design.png)




## Hypotheses
1) Novice will consider diagnostic aiding automation to be unreliable when reliability is set at 50% in both safe and dangerous environments.
2) Experts will consider diagnostic aiding automation to be reliable only in hostile environments when reliability is set at 50%.
3) Experts will consider diagnostic aiding automation to be unreliable when reliability is set to 50% in safe environments.

## Preliminary Framework/Setup

### Previous Research


![previous study design.png](./previous study design.png)




## Plan of Attack for Deliverables
* Determine our definition of novice and experts based on literature review.
* Develop questionnaire to assess whether levels of expertise find automation reliable or unreliable. Paper form or Qualtrics online survey engine?
* Determine how to administer questionnaire to participants
* Analyze data collected from Pilot Study
* Complete Github readme files
* Develop draft UCF IRB protocol for our research study





## References

* Burke, J. L., Murphy, R. R., Coovert, M. D., & Riddle, D. L. (2004). Moonlight in Miami: Field study of human-robot interaction in the context of an urban search and rescue disaster response training exercise. Human–Computer Interaction, 19(1-2), 85-116.
* Chadwick, R. A. (2008). Considerations for use of aerial views in remote unmanned ground vehicle operations. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 52, 252–256.
* Dixon, S. R., & Wickens, C. D. (2006). Automation reliability in unmanned aerial vehicle control: A reliance-compliance model of automation dependence in high workload. Human Factors, 48(3), 474-486.
* Endsley, M. R. (1995). Toward a theory of situation awareness in dynamic systems. Human Factors, 37(1), 32-64.
* Endsley, M. R. (1996). Automation and situation awareness. In R. Parasuraman & M. Mouloua (Eds.), Automation and human performance: Theory and applications, (pp. 163-181). Mahwah, NJ: Lawrence Erlbaum.Green, D. M., & Swets, J. A. (1988). Signal Detection Theory and Psychophysics. Los Altos, CA: Peninsula Publishing.
* Groom, V., & Nass, C. (2007). Can robots be teammates?: Benchmarks in human–robot teams. Interaction Studies, 8(3), 483-500.
* Horrey, W. J., Wickens, C. D., Strauss, R., Kirlik, A., & Stewart, T. R. (2009). Supporting situation assessment through attention guidance and diagnostic aiding: The benefits and costs of display enhancement on judgment skill. In A. Kirlik (Ed.), Adaptive perspectives on human- technology interaction: Methods and models for cognitive engineering and human-computer interaction (pp. 55-70). Oxford University Press.
* Madhavan, P., Wiegmann, D. A., & Lacson, F. C. (2006). Automation failures on tasks easily performed by operators undermine trust in automated aids. Human Factors, 48(2), 241-256.
* Maltz, M., & Shinar, D. (2003). New alternative methods of analyzing human behavior in cued target acquisition. Human Factors, 45(2), 281-295.
* Parasuraman, R., Sheridan, T. B., & Wickens, C. D. (2000). A model for types and levels of human interaction with automation. IEEE Transactions on systems, man, and cybernetics-Part A: Systems and Humans, 30(3), 286-297.
* Parasuraman, R., & Wickens, C.D. (2008). Humans: Still vital after all these years of automation. Human Factors, 50(3), 511-520.
* Schuster, D., Jentsch, F., Fincannon, T., & Ososky, S. (2013). The impact of type and level of automation on situation awareness and performance in human-robot interaction. In D. Harris (Ed.), Lecture Notes in Computer Science: Vol. 8019. Engineering Psychology and Cognitive Ergonomics, (pp. 252-260).
* St. John, M., & Manes, D. I. (2002). Making unreliable automation useful.
Proceedings of the Human Factors and Ergonomics Society Annual
Meeting, 46, 332-336.
* Wickens, C. D., & Dixon, S. R. (2007). The benefits of imperfect diagnostic
automation: A synthesis of the literature. Theoretical Issues in
Ergonomics Science, 8(3), 201-212.


