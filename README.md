# Research Practicum 2018 – Research Plan

> * Group Name: Mr. Robot
> * Group participants names: Blake Nguyen and Na'Kiya Russell
> * Project Title: Project Title: Human Robot Interaction (HRI): Trust and Reliance in Diagnostic Aiding Automation

## Purpose Statement
The purpose of this study is to explore the reliability threshold and its effect on human-robot teams in dangerous and safe environments. The study also seeks to compare at what level of expertise: novice or expert, relies more on diagnostic aiding technology (i.e., autonomous agent, automation) in such environments.



#### Motivation: Why is this problem interesting and relevant to the research community?

This is an interesting problem because we can understand in what environments, at what level of reliability threshold, and expertise level do humans tend to rely on technology. This can potentially benefit future research by identifying at what level of reliability does technology need to developed at (or near) to make each group (novice or expert) successful in their own right no matter the environment.

#### Proposed Solution: How do we propose to tackle this problem (that has been identified in the previous paragraphs, is interesting to the community, and has yet to be tackled by other researchers?)

In order to further investigate this area of research, we propose conducting a research experiment, specifically, a small pilot study to test some of our variables. In an effort to measure trust and reliance, we will look at the level of expertise, specifically, novice and experts to determine in what environmental conditions will they likely or less likely use diagnostic aiding automation when reliability rate is set to 50%.


#### Contributions
*	We believe our project will contribute to further advancing research regarding human-robot interaction (HRI).
*	We believe our project will contribute to the area of research that is interested in reliability in automation across human-robot teams.
*	We believe our project will provide a better understanding of reliability and willingness to work with automation at various levels of expertise.




## General Introduction

In human robot teams, there has been an increase in the use of automation being used to help teams operate tasks effectively. There any many environments where teams may have the information they need or a disruption of unexpected events occurs where unforeseen events takes place; in either situation it is best to be prepared. One factor we wanted to research is trust in diagnostic aiding automation (robots). There is a type of automation called diagnostic aiding, which involves information acquisition and analysis. When diagnostic aiding falls below 70% reliability, it is seen as not useful, and will negatively affect overall task performance.
Along with a diagnostic aid, the reliability threshold was formed to measure tasks humans are capable of performing without needing help from such technologies. Furthermore, it is stated that regardless of the reliability threshold, humans will still rely on diagnostic aiding automation in dangerous environments. However, in benign (safe) environmental conditions, the reliability level isn’t as high but it is still noted to be prominent. There is a respective interest in further investigating this area of research as to whether the level of expertise or background knowledge can play a factor in how or when humans decide to trust and rely on diagnostic aiding automation.

(Add a paraphrase of trust and level of expertise here. It shouldn't be too indepth because that material would be placed below in another section)


## Related Work

Discuss briefly about published matter that technically relates to your proposed work in 4-5 paragraphs.

##### Literature Review
###### The Human-Automation Relationship.
As tasks become more difficult for people to undertake, automation proves to be a promising alternative to decrease physical and mental workload. For the most part, automation take most complicated tasks and simplify them to aid human labor, or replace human labor altogether. People are prone to errors; automation significantly decreases human-made errors and increase efficiency. However, complete reliability is rarely achieved, and trust dwindles as reliability decreases. Nonetheless, there have been huge strides with every newer technology being developed; for example, researchers of different disciplines use automation to discover new marine or extraterrestrial life forms. Complex surgical procedures are possible with the use of automations. In spite of technical inaccuracy or other types of error during human-automation interaction, automations have progressed a plethora of technological advancements to human-robot teams.


###### Automation Errors.
The human-automation relationship contain many variables that could affect the overall performance of a set task. Dixon, Wickens, & McCarley (2007) wanted to make a clear distinction between the effects that false alarms and misses have on operators’ trust in automation. They focused on two types of automation errors: false alarms and misses. They had their participants undertake two tasks that involved aid of an automation: a tracking task and a monitoring task. Among the hypotheses of Dixon et al. (2007), believed that false alarms would harm performance by decreasing compliance and reliance. False alarms would decrease compliance by having operators distrust and ignore some of the alerts. According to the results, operators shifted their attention away from the tracking task (thereby reducing that task performance), and monitor the system’s gauge (Dixon, Wickens, & McCarley, 2007).

Dixon et al. also concluded that misses had no significant effect on compliance. It was thought that compliance and reliance were two independent constructs that can be reduced by two different automation errors. False alarms would affect only compliance. Misses would only affect reliance (Dixon, Wickens, & McCarley, 2007). However, Dixon et al. (2007), Stephen Rice (2008) provided evidence that  false alarms and misses can decrease the magnitude of both constructs, with one to a lesser degree. Rice (2008) used multiple automation aids in his study to further understand those constructs as previous models of their relationship did not seem sufficient enough.


###### Signal Detection Theory.
Wickens and Colcombe (2007) also experimented with the two constructs with a similar method, however, he offered a different outlook on the relationship between the two. Within the scope of signal detection theory, human-automation interaction is affected by the automation’s reliability (discriminate between nonevent and event) and response bias (alert and silence). In this case, Rice was focusing on the automation’s contribution in the dual tasking, and how it affects overall performance. His study supported previous claims of false alarms and misses being detrimental to task performance by lowering reliance and compliance. He explained that while both errors occurs, operators would have to divert their attention to a different task in order to double check their automation aid. This process takes time way from their assigned task and, therefore, reduce efficiency (Wickens and Colcombe, 2007). Participants interestingly continued to depend on their automation aids despite the lowered reliability. As for the automation’s response bias, results showed evidence of auditory alerts being useful for two different concurrent tasks. They’re not easy to ignore, and they require a different perceptual modality. However, different types of alerts in a false alarmed prone setting, degraded performance significantly.


###### Reliability Disclosure.
According to Wang et al. (2009), when automations aids are set to complete reliability, participants still had some distrust in them, as results showed incomplete reliance.  The objective of the study was to determine how participants appropriated their reliance on automation aids when their reliability levels were made known beforehand. Results show that while the informed group adjusted to the disclosed reliability setting, they still underperformed than expected. Apparently, this fits in line with the “sluggish beta” phenomenon, where participants don’t adjust their responses ideally when relying on the combat identification aid.


###### Single vs. Multiple Aids.
Keller and Rice (2010) shifted the focus from single aid augmentation to an automation-human dynamic where multiple aids are present. Prior research have studied the paradigm of trust and single aid setting. In this study, the objective was to determine which multiple-trust theory holds more promise. Data supported the system-wide trust theory in which operators put trust in the overall system of automation aids. Rather than having varied levels of trust, or reliance, with each aid individually (as in component-specific trust theory), operators appoint the same amount of trust for each aid despite differences in reliability (Keller D. and Rice S., 2010).


###### Adaptive Automation.
Level and type of automation was also considered to be an issue with multiple-UV systems (de Visser & Parasuraman, 2011). There are frameworks in determining how much, on what, and when to automate. Automation involvement can range from doing no work or doing all tasks. Automations can augment tasks concerning “information acquisition, information analysis, decision making, and decision implementation” (de Visser & Parasuraman, 2011). Automation could assist in critical situations, when operators’ cognitive state comes into question, or a combination of both. There would be a risk of unpredictability, but giving operators’ delegation of tasks relieve it. Contrary to previous experimental results, in a multi-UV system, imperfect automation proved to be beneficial in reliability rates as low as 30% depending on the type of target detection task (de Visser & Parasuraman, 2011). In the second experiment, adaptive automation was seen to as more useful than static automation, where operators gave the adaptive automation a high trust rating when it intervened at the opportune moment. In this case, the automation intervened when task load was high.


While there were setbacks with automation aids, there have been documented benefits and stability with human-automation interactions. Levels of trust is an important indicator of operators’ dependence of automations. The ability to perceive patterns and orientation relative to other objects and the switch attention from one task to another are important markers, too. Newer research are focusing on how many automations operators can oversee, with numbers ranging from one to sixteen. Research on adaptive automation suggests that operators’ tend to trust automations when workload is heavy and when the automation intervenes at the most critical event. Major work has been done to definitively establish the relationship between compliance and reliance, however, limitations in experiments restrict the process. Nonetheless, both constructs seem to be two different forms of trust, and they are both affected by two different automation errors.


###### Trust

###### Level of Expertise

(Add a paragraph on trust and a paragraph on levels of expertise here.)



## Research Questions
* Is there a difference in perception of reliable and unreliable diagnostic aiding automation in both safe and dangerous environments when comparing levels of expertise (novice and experts)?
* Do novice consider diagnostic aiding automation to be reliable or unreliable as opposed to an expert operator when automation reliability rate is set to 50% in both benign and hostile environments?



## Hypotheses
* H1: It is hypothesized that novice participants in the 50% reliability condition will have lower levels of trust in an autonomous robot than expert participants.
* H2: It is hypothesized that novice participants in both the safe and dangerous environments will have lower levels of trust in an autonomous robot than expert participants.
* H3: It is hypothesized that expert participants with both 50% reliability condition and safe environment will have lower levels of trust in an autonomous robot like novice participants.
* H4: It is hypothesized that expert participants with both 50% reliability condition and dangerous environment will have higher levels of trust in an autonomous robot unlike novice participants.


## Methods
#### Participants
Participants involved in this study will have to be students who are enrolled in a psychology class at the University of Central Florida and are over the age of 18. Participants will have to demonstrate eligibility (class registration) by signing up for Sona Systems and completing a pre-screening measure provided by Sona Systems (age). This pre-screening measure will screen students for age and gender such that only students who are 18 years old and above will be able to sign up to participate in our study. Researchers will not attempt to recruit persons identified as being part of a vulnerable population (e.g., children, prisoners, mentally disabled persons).


#### Design
A 2 (Environment: Benign (Safe) or Dangerous) x 2 (Level of Expertise: Novice or Expert) within-subjects study design. Please note that the reliability rate of 50% is not considered a part of the design because it is kept constant at 50%. Our independent variable is environment, which we've determined as being either benign or dangerous. An example of a safe environment would be assisting people who are blind. An example of a dangerous environment would be a terrorist attack. Based on our literature review on levels of expertise, that will help us determine how we define the differences between a novice and an expert.


#### Preliminary Framework/Setup
Our framework is modeled after previously published research on diagnostic aiding automation in various environmental conditions. This framework was setup but does offer some areas of research that can be further improved/advanced. In the previous work, the researchers looked at reliability rate and environments. The previous study design was a 2 (Environment: Benign or Dangerous) x 2 (Reliability: Low or High) between-subjects design. This framework provided us with the basis of our current study design. We were able to change one variable and manipulate the condition to further investigate whether or not levels of expertise play a part in determining if that changes the willingness to use diagnostic aiding automation in the same environmental conditions as in the previous study. Please refer to our png files of the study design for both the previous study design and our current study design. (Blake is having issues with the png images so they are not appearing on the Github repository but are in the readme file edit via Haroopad. She'll most likely stop by Dr. Kider's office to troubleshoot the issue were having with images/models/etc.)

#### Model
(Add model image here)
(Discuss previous models such as the one Dr. Kider sent us on trust)
(Discuss our model and explanation of the interactions)

#### Material


#### Procedure


## Results
Expected Results
Data Analysis
(Add all written material on data analysis (i.e., mean, standard deviation, MANOVA, ANOVA) and why those are important statistics to run.)



## Discussion




## Completed Plan of Action Items
* Determine our definition of novice and experts based on our literature review.
* Develop stimuli which includes reading materials and pictures describing the purpose, capabilities, reliability and operational environment of our diagnostic aiding automation.
* Develop a qualitative questionnaire to assess whether levels of expertise find automation reliable or unreliable. Paper form or Qualtrics online survey engine?
* Determine how to administer questionnaire to participants
* Analyze data collected from Pilot Study
* Complete Github readme files
* Develop draft UCF IRB protocol for our research study



## Future Plan of Action Items



## References

* de Visser, E., & Parasuraman, R. (2011). Adaptive aiding of human-robot teaming: Effects of imperfect automation on performance, trust, and workload. Journal of Cognitive Engineering and Decision Making, 5(2), 209-231.
*  Dixon, S. R., & Wickens, C. D. (2006). Automation reliability in unmanned aerial vehicle control: A reliance-compliance model of automation dependence in high workload. Human Factors, 48(3), 474-486.
* Dixon, S. R., Wickens, C. D., & McCarley, J. S. (2007). On the independence of compliance and reliance: Are automation false alarms worse than misses?. Human factors, 49(4), 564-572.
* Endsley, M. R. (1995). Toward a theory of situation awareness in dynamic systems. Human Factors, 37(1), 32-64.
* Endsley, M. R. (1996). Automation and situation awareness. In R. Parasuraman & M. Mouloua (Eds.), Automation and human performance: Theory and applications, (pp. 163-181). Mahwah, NJ: Lawrence Erlbaum.Green, D. M., & Swets, J. A. (1988). Signal Detection Theory and Psychophysics. Los Altos, CA: Peninsula Publishing.
* Madhavan, P., Wiegmann, D. A., & Lacson, F. C. (2006). Automation failures on tasks easily performed by operators undermine trust in automated aids. Human Factors, 48(2), 241-256.
* Maltz, M., & Shinar, D. (2003). New alternative methods of analyzing human behavior in cued target acquisition. Human Factors, 45(2), 281-295.
* Parasuraman, R., Sheridan, T. B., & Wickens, C. D. (2000). A model for types and levels of human interaction with automation. IEEE Transactions on systems, man, and cybernetics-Part A: Systems and Humans, 30(3), 286-297.
* Parasuraman, R., & Wickens, C.D. (2008). Humans: Still vital after all these years of automation. Human Factors, 50(3), 511-520.
* Rice, S., & Geels, K. (2010). Using system-wide trust theory to make predictions about dependence on four diagnostic aids. The Journal of general psychology, 137(4), 362-375.
* Schuster, D., Jentsch, F., Fincannon, T., & Ososky, S. (2013). The impact of type and level of automation on situation awareness and performance in human-robot interaction. In D. Harris (Ed.), Lecture Notes in Computer Science: Vol. 8019. Engineering Psychology and Cognitive Ergonomics, (pp. 252-260). Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 46, 332-336.
* Wang, L., Jamieson, G. A., & Hollands, J. G. (2009). Trust and reliance on an automated combat identification system. Human factors, 51(3), 281-291.
* Wickens, C. D., & Dixon, S. R. (2007). The benefits of imperfect diagnostic automation: A synthesis of the literature. Theoretical Issues in Ergonomics Science, 8(3), 201-212.

(Na'Kiya add all of the references you've included here in alphabetical order)








